---
layout: post
title: A "Good" Data Scientist
date: '2021-05-25 22:50'
excerpt: With all the different researchers and data scientists, how does one stand out?
comments: false
---

So a few weeks back, I started working for Amazon under the ProServe Data and Machine Learning group. Basically, we're a group who joins a project to do the Machine Learning bit then leave the project. We get to do work on really cool projects but aren't fully committed to the project. So far the experience has been great even though I've been mostly going through the onboarding videos and trainings. An interesting part of my onboarding is that I was able to sign up as a mentee and I could be paired with another senior in the firm who could guide me. I was mostly interested to learn more about research opportunities within the group and how I could get my hands on those opportunities. Then the conversation sort of drifted off and we started talking about what makes a data scientist better than the average.

My mentor told me to strive to be someone who can "bridge the gap between textbook models and real world data". At first, it didn't make sense. Isn't the role of a data scientist to pass real world data into these textbook models? Every data scientist who would strive to be the best in what they do can probably use the state of the art model in the field that they specialize in. I was no different. I highlighted and filled most of the projects in my resume with the ones that had the most buzzwords in it. I used "Transformer models" for "NER" and "Information Retrieval" and leveraged "Attention Mechanisms" and blah blah blah. But isn't that sort of where the field is going to? BERT is becoming the baseline for NLP tasks. We "expect" this language model to outperform any traditional machine learning algorithm before it such as word2vec or bag of words models. Apparently, being able to use the state of the art doesn't mean that you're a great data scientist.

What he meant (or more accurately, what I understood) by "bridge the gap" was understanding the data and model well enough to know if its the right solution. It's not enough to know if we have the time to process the data to pass through the model. We have to really know the data to the point that you understand the data (how it looks, the size, the structure), the users who will interact with it (their preference, their behavior, their task), and the problem (what problem are we trying to solve, assumptions, etc.). From what I've seen and what I've previously done, these are questions that we skip to jump on the ML development aspect of things. We forget that all of these questions are essential for our decision making in the model development side.

Speaking of models, should we also always use state of the art models? Nope. This is something I learned to reiterate in my previous role where ML seems to be the approach to everything but a simpler rule based solution can do the trick. This was probably one of my proudest moments as a Data Scientist, but I learned to write regular expressions and created this hierarchy class to parse our data. From there, we didn't just dump a block of text and expect the model to do all of the work but we've created a way to filter the document though its headers and subheaders. Why am I telling this story again? Oh right. It's to point out that state of the art models aren't always the best solutions. I'll probably write another post about what I learned with regular expressions some other time but that's future Prince's problems.

Let's take a step back. By bridging the gap between models and data, our role as data scientists is to understand what the right "bridge" is. And for us to know what the right "bridge" is, we have to understand the landmark of both the data and the model. Our role is to understand what the best way is to connect a model trained on a specific data. Understanding what the assumptions of the model are and if it overlaps or differs from the specific data. What would be the thought process of a particular user when they try to perform the task that we're trying to automate. What are the constraints of the algorithm that prevents us from fully using it in this dataset. These are the questions that we tend to skip but these are the questions that a "good" data scientist would ask.

It's a lot but at the same time, it's awesome. ML is not just a mathy topic that only nerds love. Being a data scientist, like any other job, means we have to solve problems. And there's no one solution that solves everything (*cough* AutoML *cough*). Solving these problems is not like picking a set of clothes that you think you'll look best. It's like playing sudoku on at least a medium difficulty. It's an iterative process of trying something in this section, see if it works, then repeat. To solve that problem, we not only look at one specific box in the entire 9 x 9 grid. We look at how our answer in this tile affects the horizontal, vertical, and 3 x 3 grid it's under. And I think that's where people can begin to find and appreciate the beauty of data science.

So yeah. If we want to be a "good" data scientist, we should start asking more questions. Start questioning the algorithms we just use in our day to day problems. We question things not because we're against it but because by resolving our doubt in it, we can better rely on these solutions.
